{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"\"\n",
    "os.environ[\"GROQ_API_KEY\"] = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733e3a4",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/boltuix/conll2025-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bba3f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['split', 'tokens', 'ner_tags'],\n",
      "        num_rows: 143709\n",
      "    })\n",
      "})\n",
      "Train example:\n",
      "{'split': 'train', 'tokens': ['In', 'recent', 'years', ',', 'advanced', 'education', 'for', 'professionals', 'has', 'become', 'a', 'hot', 'topic', 'in', 'the', 'business', 'community', '.'], 'ner_tags': ['O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"boltuix/conll2025-ner\")\n",
    "\n",
    "print(f\"Dataset:\\n{ds}\\nTrain example:\\n{ds['train'][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26cec429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tokens: ['In', 'recent', 'years', ',', 'advanced', 'education', 'for', 'professionals', 'has', 'become', 'a', 'hot', 'topic', 'in', 'the', 'business', 'community', '.']\n",
      "First BIO tags: ['O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Formatted NER example:\n",
      "{'text': 'In recent years, advanced education for professionals has become a hot topic in the business community.', 'ner_tags': [{'recent years': 'DATE'}]}\n"
     ]
    }
   ],
   "source": [
    "from sugardata.utility.ner import NERExampleFormatter\n",
    "\n",
    "WANTED_LABELS = [\"PERSON\", \"DATE\", \"ORG\", \"LOC\", \"EVENT\"]\n",
    "FULL_TO_SHORT = {\n",
    "    \"PERSON\": \"PER\",\n",
    "    \"DATE\": \"DATE\",\n",
    "    \"ORG\": \"ORG\",\n",
    "    \"LOC\": \"LOC\",\n",
    "    \"EVENT\": \"EVENT\"\n",
    "}\n",
    "\n",
    "tokens_list = [example[\"tokens\"] for example in ds[\"train\"]]\n",
    "bio_tags_list = [example[\"ner_tags\"] for example in ds[\"train\"]]\n",
    "\n",
    "print(f\"First tokens: {tokens_list[1]}\")\n",
    "print(f\"First BIO tags: {bio_tags_list[1]}\")\n",
    "\n",
    "formatter = NERExampleFormatter(wanted_labels=WANTED_LABELS, full_to_short=FULL_TO_SHORT)\n",
    "examples = formatter.build_from_bio(tokens_list, bio_tags_list)\n",
    "\n",
    "print(f\"Formatted NER example:\\n{examples[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c65aa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Results (first 5 examples):\n",
      "{'text': \"so anyway I'm just so stressed out right now that I feel like crying\", 'ner_tags': []}\n",
      "----\n",
      "{'text': 'The Israeli spokesman said there were two simultaneous attacks at an intersection.', 'ner_tags': []}\n",
      "----\n",
      "{'text': 'Talks may resume after the Israeli elections.', 'ner_tags': []}\n",
      "----\n",
      "{'text': 'Does he make any sounds /?', 'ner_tags': []}\n",
      "----\n",
      "{'text': 'They offer these suggestions:', 'ner_tags': []}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample_count = 5_000\n",
    "\n",
    "sample_indices = random.sample(range(len(examples)), sample_count)\n",
    "\n",
    "sampled_examples = [examples[i] for i in sample_indices]\n",
    "\n",
    "print(f\"Sampled Results (first 5 examples):\")\n",
    "\n",
    "for res in sampled_examples[:5]:\n",
    "    print(res)\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88f05945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sugardata import localize_ner_data\n",
    "\n",
    "\n",
    "examples = sampled_examples\n",
    "language = \"Turkish\"\n",
    "model = \"gpt-4o-mini\"\n",
    "vendor = \"openai\"\n",
    "tokenizer = \"dbmdz/bert-base-turkish-cased\"\n",
    "entity_labels = {\"PER\": (1, 2), \"ORG\": (3, 4), \"LOC\": (5, 6), \"DATE\": (7, 8), \"EVENT\": (9, 10)}\n",
    "batch_size = 32\n",
    "verbose = True\n",
    "\n",
    "\n",
    "results_example = localize_ner_data(\n",
    "    examples=examples[:10],\n",
    "    language=language,\n",
    "    model=model,\n",
    "    vendor=vendor,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    entity_labels=entity_labels,\n",
    "    export_type=\"default\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa7d37b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Localization Results (first 2 examples) compared to examples:\n",
      "Example 1:\n",
      "Original: {'text': \"so anyway I'm just so stressed out right now that I feel like crying\", 'ner_tags': []}\n",
      "Localized: {'index': 0, 'localized_text': 'Her neyse, şu anda o kadar stresli hissediyorum ki ağlamak istiyorum.', 'localized_word_mappings': {}, 'tokens': ['Her', 'neyse', ',', 'şu', 'anda', 'o', 'kadar', 'stres', '##li', 'hissediyorum', 'ki', 'ağ', '##lamak', 'istiyorum', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'ner_tag_labels': {'PER': (1, 2), 'ORG': (3, 4), 'LOC': (5, 6), 'DATE': (7, 8), 'EVENT': (9, 10)}}\n",
      "----\n",
      "Example 2:\n",
      "Original: {'text': 'The Israeli spokesman said there were two simultaneous attacks at an intersection.', 'ner_tags': []}\n",
      "Localized: {'index': 1, 'localized_text': 'İsrailli sözcü, bir kavşakta iki eş zamanlı saldırı olduğunu söyledi.', 'localized_word_mappings': {}, 'tokens': ['İsrailli', 'sözcü', ',', 'bir', 'kavşak', '##ta', 'iki', 'eş', 'zamanlı', 'saldırı', 'olduğunu', 'söyledi', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'ner_tag_labels': {'PER': (1, 2), 'ORG': (3, 4), 'LOC': (5, 6), 'DATE': (7, 8), 'EVENT': (9, 10)}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "print(f\"Localization Results (first 2 examples) compared to examples:\")\n",
    "for i in range(2):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(\"Original:\", examples[i])\n",
    "    print(\"Localized:\", results_example[i])\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6ce474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sugardata import localize_ner_data_async\n",
    "\n",
    "local_example_async = await localize_ner_data_async(\n",
    "    examples=examples[:10],\n",
    "    language=language,\n",
    "    model=model,\n",
    "    vendor=vendor,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    entity_labels=entity_labels,\n",
    "    export_type=\"default\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e9e109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Localization Results (first 2 examples) compared to examples:\n",
      "Example 1:\n",
      "Original: {'text': \"so anyway I'm just so stressed out right now that I feel like crying\", 'ner_tags': []}\n",
      "Localized: {'index': 0, 'localized_text': 'Yani şu an o kadar stresli hissediyorum ki ağlamak istiyorum.', 'localized_word_mappings': {}, 'tokens': ['Yani', 'şu', 'an', 'o', 'kadar', 'stres', '##li', 'hissediyorum', 'ki', 'ağ', '##lamak', 'istiyorum', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'ner_tag_labels': {'PER': (1, 2), 'ORG': (3, 4), 'LOC': (5, 6), 'DATE': (7, 8), 'EVENT': (9, 10)}}\n",
      "----\n",
      "Example 2:\n",
      "Original: {'text': 'The Israeli spokesman said there were two simultaneous attacks at an intersection.', 'ner_tags': []}\n",
      "Localized: {'index': 4, 'localized_text': 'Bu önerileri sunuyorlar:', 'localized_word_mappings': {}, 'tokens': ['Bu', 'önerileri', 'sunuyor', '##lar', ':'], 'ner_tags': [0, 0, 0, 0, 0], 'ner_tag_labels': {'PER': (1, 2), 'ORG': (3, 4), 'LOC': (5, 6), 'DATE': (7, 8), 'EVENT': (9, 10)}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "print(f\"Localization Results (first 2 examples) compared to examples:\")\n",
    "for i in range(2):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(\"Original:\", examples[i])\n",
    "    print(\"Localized:\", local_example_async[i])\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a9ff6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini] Starting text generation: 40 batches\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Starting text generation: 40 batches\n",
      "[models/gemini-2.0-flash-lite] Starting text generation: 40 batches\n",
      "[gemma3:12b] Starting text generation: 40 batches\n",
      "[models/gemini-2.0-flash-lite] Generated batch 1/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 1/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 2/40\n",
      "[gpt-4o-mini] Generated batch 1/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 2/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 3/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 4/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 3/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 5/40\n",
      "[gpt-4o-mini] Generated batch 2/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 4/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 6/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 5/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 7/40\n",
      "[gpt-4o-mini] Generated batch 3/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 8/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 6/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 9/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 7/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 8/40\n",
      "[gpt-4o-mini] Generated batch 4/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 9/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 10/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 10/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 11/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 12/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 11/40\n",
      "[gemma3:12b] Generated batch 1/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 13/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 14/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 12/40\n",
      "[gpt-4o-mini] Generated batch 5/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 15/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 13/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 16/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 17/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 14/40\n",
      "[gpt-4o-mini] Generated batch 6/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 18/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 15/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 19/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 20/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 16/40\n",
      "[gpt-4o-mini] Generated batch 7/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 21/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 22/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 17/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 23/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 24/40\n",
      "[gemma3:12b] Generated batch 2/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 18/40\n",
      "[gpt-4o-mini] Generated batch 8/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 19/40\n",
      "[gpt-4o-mini] Generated batch 9/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 20/40\n",
      "[gpt-4o-mini] Generated batch 10/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 25/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 21/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 26/40\n",
      "[gpt-4o-mini] Generated batch 11/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 22/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 27/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 28/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 29/40\n",
      "[gpt-4o-mini] Generated batch 12/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 23/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 30/40\n",
      "[gemma3:12b] Generated batch 3/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 31/40\n",
      "[gpt-4o-mini] Generated batch 13/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 24/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 32/40\n",
      "[gpt-4o-mini] Generated batch 14/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 33/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 34/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 25/40\n",
      "[gpt-4o-mini] Generated batch 15/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 35/40\n",
      "[gpt-4o-mini] Generated batch 16/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 26/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 36/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 27/40\n",
      "[gpt-4o-mini] Generated batch 17/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 37/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 38/40\n",
      "[gemma3:12b] Generated batch 4/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 28/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 39/40\n",
      "[models/gemini-2.0-flash-lite] Generated batch 40/40\n",
      "[models/gemini-2.0-flash-lite] Text generation complete\n",
      "[models/gemini-2.0-flash-lite] Starting entity labeling: 1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 50/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 100/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 150/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 200/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 250/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 300/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 350/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 400/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 450/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 500/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 550/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 600/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 650/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 700/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 750/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 800/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 850/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 900/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 950/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 1000/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 1050/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 1100/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 1150/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 1200/1243 records\n",
      "[models/gemini-2.0-flash-lite] Labeled 1243/1243 records\n",
      "[models/gemini-2.0-flash-lite] Entity labeling complete\n",
      "[gpt-4o-mini] Generated batch 18/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 29/40\n",
      "[gpt-4o-mini] Generated batch 19/40\n",
      "[gpt-4o-mini] Generated batch 20/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 30/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 31/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 32/40\n",
      "[gpt-4o-mini] Generated batch 21/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 33/40\n",
      "[gemma3:12b] Generated batch 5/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 34/40\n",
      "[gpt-4o-mini] Generated batch 22/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 35/40\n",
      "[gpt-4o-mini] Generated batch 23/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 36/40\n",
      "[gpt-4o-mini] Generated batch 24/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 37/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 38/40\n",
      "[gpt-4o-mini] Generated batch 25/40\n",
      "[gemma3:12b] Generated batch 6/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 39/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Generated batch 40/40\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Text generation complete\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Starting entity labeling: 1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 50/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 100/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 150/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 200/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 250/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 300/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 350/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 400/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 450/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 500/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 550/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 600/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 650/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 700/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 750/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 800/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 850/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 900/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 950/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 1000/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 1050/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Labeled 1068/1068 records\n",
      "[meta-llama/llama-4-scout-17b-16e-instruct] Entity labeling complete\n",
      "[gpt-4o-mini] Generated batch 26/40\n",
      "[gpt-4o-mini] Generated batch 27/40\n",
      "[gpt-4o-mini] Generated batch 28/40\n",
      "[gpt-4o-mini] Generated batch 29/40\n",
      "[gpt-4o-mini] Generated batch 30/40\n",
      "[gpt-4o-mini] Generated batch 31/40\n",
      "[gemma3:12b] Generated batch 7/40\n",
      "[gpt-4o-mini] Generated batch 32/40\n",
      "[gpt-4o-mini] Generated batch 33/40\n",
      "[gemma3:12b] Generated batch 8/40\n",
      "[gpt-4o-mini] Generated batch 34/40\n",
      "[gpt-4o-mini] Generated batch 35/40\n",
      "[gpt-4o-mini] Generated batch 36/40\n",
      "[gpt-4o-mini] Generated batch 37/40\n",
      "[gemma3:12b] Generated batch 9/40\n",
      "[gpt-4o-mini] Generated batch 38/40\n",
      "[gpt-4o-mini] Generated batch 39/40\n",
      "[gpt-4o-mini] Generated batch 40/40\n",
      "[gpt-4o-mini] Text generation complete\n",
      "[gpt-4o-mini] Starting entity labeling: 1249 records\n",
      "[gpt-4o-mini] Labeled 50/1249 records\n",
      "[gpt-4o-mini] Labeled 100/1249 records\n",
      "[gpt-4o-mini] Labeled 150/1249 records\n",
      "[gpt-4o-mini] Labeled 200/1249 records\n",
      "[gpt-4o-mini] Labeled 250/1249 records\n",
      "[gpt-4o-mini] Labeled 300/1249 records\n",
      "[gpt-4o-mini] Labeled 350/1249 records\n",
      "[gpt-4o-mini] Labeled 400/1249 records\n",
      "[gpt-4o-mini] Labeled 450/1249 records\n",
      "[gpt-4o-mini] Labeled 500/1249 records\n",
      "[gpt-4o-mini] Labeled 550/1249 records\n",
      "[gpt-4o-mini] Labeled 600/1249 records\n",
      "[gpt-4o-mini] Labeled 650/1249 records\n",
      "[gpt-4o-mini] Labeled 700/1249 records\n",
      "[gpt-4o-mini] Labeled 750/1249 records\n",
      "[gpt-4o-mini] Labeled 800/1249 records\n",
      "[gpt-4o-mini] Labeled 850/1249 records\n",
      "[gpt-4o-mini] Labeled 900/1249 records\n",
      "[gpt-4o-mini] Labeled 950/1249 records\n",
      "[gpt-4o-mini] Labeled 1000/1249 records\n",
      "[gpt-4o-mini] Labeled 1050/1249 records\n",
      "[gpt-4o-mini] Labeled 1100/1249 records\n",
      "[gpt-4o-mini] Labeled 1150/1249 records\n",
      "[gpt-4o-mini] Labeled 1200/1249 records\n",
      "[gpt-4o-mini] Labeled 1249/1249 records\n",
      "[gpt-4o-mini] Entity labeling complete\n",
      "[gemma3:12b] Generated batch 10/40\n",
      "[gemma3:12b] Generated batch 11/40\n",
      "[gemma3:12b] Generated batch 12/40\n",
      "[gemma3:12b] Generated batch 13/40\n",
      "[gemma3:12b] Generated batch 14/40\n",
      "[gemma3:12b] Generated batch 15/40\n",
      "[gemma3:12b] Generated batch 16/40\n",
      "[gemma3:12b] Generated batch 17/40\n",
      "[gemma3:12b] Generated batch 18/40\n",
      "[gemma3:12b] Generated batch 19/40\n",
      "[gemma3:12b] Generated batch 20/40\n",
      "[gemma3:12b] Generated batch 21/40\n",
      "[gemma3:12b] Generated batch 22/40\n",
      "[gemma3:12b] Generated batch 23/40\n",
      "[gemma3:12b] Generated batch 24/40\n",
      "[gemma3:12b] Generated batch 25/40\n",
      "[gemma3:12b] Generated batch 26/40\n",
      "[gemma3:12b] Generated batch 27/40\n",
      "[gemma3:12b] Generated batch 28/40\n",
      "[gemma3:12b] Generated batch 29/40\n",
      "[gemma3:12b] Generated batch 30/40\n",
      "[gemma3:12b] Generated batch 31/40\n",
      "[gemma3:12b] Generated batch 32/40\n",
      "[gemma3:12b] Generated batch 33/40\n",
      "[gemma3:12b] Generated batch 34/40\n",
      "[gemma3:12b] Generated batch 35/40\n",
      "[gemma3:12b] Generated batch 36/40\n",
      "[gemma3:12b] Generated batch 37/40\n",
      "[gemma3:12b] Generated batch 38/40\n",
      "[gemma3:12b] Generated batch 39/40\n",
      "[gemma3:12b] Generated batch 40/40\n",
      "[gemma3:12b] Text generation complete\n",
      "[gemma3:12b] Starting entity labeling: 1183 records\n",
      "[gemma3:12b] Labeled 50/1183 records\n",
      "[gemma3:12b] Labeled 100/1183 records\n",
      "[gemma3:12b] Labeled 150/1183 records\n",
      "[gemma3:12b] Labeled 200/1183 records\n",
      "[gemma3:12b] Labeled 250/1183 records\n",
      "[gemma3:12b] Labeled 300/1183 records\n",
      "[gemma3:12b] Labeled 350/1183 records\n",
      "[gemma3:12b] Labeled 400/1183 records\n",
      "[gemma3:12b] Labeled 450/1183 records\n",
      "[gemma3:12b] Labeled 500/1183 records\n",
      "[gemma3:12b] Labeled 550/1183 records\n",
      "[gemma3:12b] Labeled 600/1183 records\n",
      "[gemma3:12b] Labeled 650/1183 records\n",
      "[gemma3:12b] Labeled 700/1183 records\n",
      "[gemma3:12b] Labeled 750/1183 records\n",
      "[gemma3:12b] Labeled 800/1183 records\n",
      "[gemma3:12b] Labeled 850/1183 records\n",
      "[gemma3:12b] Labeled 900/1183 records\n",
      "[gemma3:12b] Labeled 950/1183 records\n",
      "[gemma3:12b] Labeled 1000/1183 records\n",
      "[gemma3:12b] Labeled 1050/1183 records\n",
      "[gemma3:12b] Labeled 1100/1183 records\n",
      "[gemma3:12b] Labeled 1150/1183 records\n",
      "[gemma3:12b] Labeled 1183/1183 records\n",
      "[gemma3:12b] Entity labeling complete\n"
     ]
    }
   ],
   "source": [
    "from sugardata import localize_ner_data_multi_vendor_async\n",
    "\n",
    "examples = sampled_examples\n",
    "batch_size = 32\n",
    "verbose = True\n",
    "\n",
    "vendors = {\n",
    "    \"openai\": \"gpt-4o-mini\",\n",
    "    \"gemini\": \"gemini-2.0-flash-lite\",\n",
    "    \"groq\": \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    \"ollama\": \"gemma3:12b\",\n",
    "}\n",
    "\n",
    "results = await localize_ner_data_multi_vendor_async(\n",
    "    examples=sampled_examples,\n",
    "    language=language,\n",
    "    tokenizer=tokenizer,\n",
    "    entity_labels=entity_labels,\n",
    "    batch_size=batch_size,\n",
    "    vendors=vendors,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c02fb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total localized examples: 4743\n"
     ]
    }
   ],
   "source": [
    "local_data = []\n",
    "for vendor, vendor_results in results.items():\n",
    "    local_data.extend(vendor_results)\n",
    "\n",
    "print(f\"Total localized examples: {len(local_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0e47287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "NER_LABELS: Dict[str, Tuple[int,int]] = {\n",
    "    'PER': (1, 2),\n",
    "    'ORG':    (3, 4),\n",
    "    'LOC':    (5, 6),\n",
    "    'DATE':   (7, 8),\n",
    "    'EVENT':  (9,10),\n",
    "}\n",
    "\n",
    "id2label = {0: \"O\"}\n",
    "for ent, (b_id, i_id) in NER_LABELS.items():\n",
    "    id2label[b_id] = f\"B-{ent}\"\n",
    "    id2label[i_id] = f\"I-{ent}\"\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "num_labels = max(id2label.keys()) + 1\n",
    "\n",
    "\n",
    "assert all(isinstance(x[\"tokens\"], list) and isinstance(x[\"ner_tags\"], list) for x in local_data)\n",
    "assert all(len(x[\"tokens\"]) == len(x[\"ner_tags\"]) for x in local_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c497a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'localized_text', 'localized_word_mappings', 'tokens', 'ner_tags', 'ner_tag_labels'],\n",
      "        num_rows: 3794\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'localized_text', 'localized_word_mappings', 'tokens', 'ner_tags', 'ner_tag_labels'],\n",
      "        num_rows: 474\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'localized_text', 'localized_word_mappings', 'tokens', 'ner_tags', 'ner_tag_labels'],\n",
      "        num_rows: 475\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train / validation / test split (80/10/10)\n",
    "random.seed(42)\n",
    "idx = list(range(len(local_data)))\n",
    "train_idx, tmp_idx = train_test_split(idx, test_size=0.2, random_state=42, shuffle=True)\n",
    "valid_idx, test_idx = train_test_split(tmp_idx, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "train_data = [local_data[i] for i in train_idx]\n",
    "valid_data = [local_data[i] for i in valid_idx]\n",
    "test_data  = [local_data[i] for i in test_idx]\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_data),\n",
    "    \"validation\": Dataset.from_list(valid_data),\n",
    "    \"test\": Dataset.from_list(test_data),\n",
    "})\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96ff9b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a6472094bd4cb0a9e8a238e9c42b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3794 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76caff132e0e4669984821811bddc166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/474 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e68cf9b1124a688465c14dff917e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/475 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'localized_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3794\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'localized_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 474\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'localized_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 475\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"dbmdz/bert-base-turkish-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "CLS_ID = tokenizer.cls_token_id\n",
    "SEP_ID = tokenizer.sep_token_id\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "def encode_example(example, max_length=256):\n",
    "    # Convert your piecewise tokens → ids (unknown pieces become [UNK])\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(example[\"tokens\"])\n",
    "    attention_mask = [1]*len(input_ids)\n",
    "\n",
    "    # Add special tokens [CLS] ... [SEP]\n",
    "    input_ids = [CLS_ID] + input_ids + [SEP_ID]\n",
    "    attention_mask = [1] + attention_mask + [1]\n",
    "\n",
    "    # Shift labels: add -100 for [CLS] and [SEP] so they are ignored by the loss\n",
    "    labels = [-100] + example[\"ner_tags\"] + [-100]\n",
    "\n",
    "    # Truncate if needed (keep room already considered)\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids   = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    # No padding here; DataCollator will pad dynamically per batch\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"localized_text\": example.get(\"localized_text\", \"\"),\n",
    "        \"index\": example.get(\"index\", -1),\n",
    "    }\n",
    "\n",
    "encoded_ds = ds.map(encode_example, remove_columns=ds[\"train\"].column_names)\n",
    "\n",
    "print(f\"Encoded dataset:\\n{encoded_ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c95b8521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True, max_length=256)\n",
    "\n",
    "# Build a function to convert label ids back to strings (excluding -100 and specials)\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    batch_preds = []\n",
    "    batch_refs = []\n",
    "\n",
    "    for pred, lab in zip(preds, label_ids):\n",
    "        true_preds = []\n",
    "        true_labels = []\n",
    "        for p, l in zip(pred, lab):\n",
    "            if l == -100:\n",
    "                continue\n",
    "            true_preds.append(id2label[p])\n",
    "            true_labels.append(id2label[l])\n",
    "        batch_preds.append(true_preds)\n",
    "        batch_refs.append(true_labels)\n",
    "    return batch_preds, batch_refs\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds_list, refs_list = align_predictions(predictions, labels)\n",
    "    results = seqeval.compute(predictions=preds_list, references=refs_list)\n",
    "    # Friendly flatten of main scores\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\":    results[\"overall_recall\"],\n",
    "        \"f1\":        results[\"overall_f1\"],\n",
    "        \"accuracy\":  results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/bert-tr-ner\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",   # set \"wandb\" etc. if you want\n",
    "    seed=42,\n",
    "    dataloader_num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a756a72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26675/1448628327.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='714' max='714' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [714/714 00:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.123300</td>\n",
       "      <td>0.079880</td>\n",
       "      <td>0.639405</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.697769</td>\n",
       "      <td>0.972706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>0.087053</td>\n",
       "      <td>0.625899</td>\n",
       "      <td>0.776786</td>\n",
       "      <td>0.693227</td>\n",
       "      <td>0.970422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>0.091780</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.972597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=714, training_loss=0.11032439196477083, metrics={'train_runtime': 16.1785, 'train_samples_per_second': 703.524, 'train_steps_per_second': 44.133, 'total_flos': 309988498909440.0, 'train_loss': 0.11032439196477083, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_ds[\"train\"],\n",
    "    eval_dataset=encoded_ds[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,     # keeps special tokens, etc.\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddbc22f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08436267077922821, 'eval_precision': 0.7209302325581395, 'eval_recall': 0.7833935018050542, 'eval_f1': 0.7508650519031141, 'eval_accuracy': 0.9747692307692307, 'eval_runtime': 0.2368, 'eval_samples_per_second': 2005.81, 'eval_steps_per_second': 126.683, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.evaluate(encoded_ds[\"test\"])\n",
    "print(test_metrics)  # precision / recall / f1 / accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ad65e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/yeniguno/projects/sugardata/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATE': {'precision': np.float64(0.5394736842105263), 'recall': np.float64(0.6721311475409836), 'f1': np.float64(0.5985401459854015), 'number': np.int64(61)}, 'EVENT': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(2)}, 'LOC': {'precision': np.float64(0.5), 'recall': np.float64(0.4444444444444444), 'f1': np.float64(0.47058823529411764), 'number': np.int64(9)}, 'ORG': {'precision': np.float64(0.7652173913043478), 'recall': np.float64(0.8), 'f1': np.float64(0.7822222222222223), 'number': np.int64(110)}, 'PER': {'precision': np.float64(0.8316831683168316), 'recall': np.float64(0.8842105263157894), 'f1': np.float64(0.8571428571428571), 'number': np.int64(95)}, 'overall_precision': np.float64(0.7209302325581395), 'overall_recall': np.float64(0.7833935018050542), 'overall_f1': np.float64(0.7508650519031141), 'overall_accuracy': 0.9747692307692307}\n"
     ]
    }
   ],
   "source": [
    "# Get raw predictions for per-class report\n",
    "predictions = trainer.predict(encoded_ds[\"test\"])\n",
    "preds_list, refs_list = align_predictions(predictions.predictions, predictions.label_ids)\n",
    "report = evaluate.load(\"seqeval\").compute(predictions=preds_list, references=refs_list)\n",
    "print(report)  # includes per-entity scores like B-PERSON/I-PERSON aggregated as PER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8160814a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('outputs/bert-tr-ner/final/tokenizer_config.json',\n",
       " 'outputs/bert-tr-ner/final/special_tokens_map.json',\n",
       " 'outputs/bert-tr-ner/final/vocab.txt',\n",
       " 'outputs/bert-tr-ner/final/added_tokens.json',\n",
       " 'outputs/bert-tr-ner/final/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"outputs/bert-tr-ner/final\")\n",
    "tokenizer.save_pretrained(\"outputs/bert-tr-ner/final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30a66b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9883959),\n",
       "  'word': 'Fumio Kishida',\n",
       "  'start': 18,\n",
       "  'end': 31},\n",
       " {'entity_group': 'DATE',\n",
       "  'score': np.float32(0.83149445),\n",
       "  'word': 'bugün',\n",
       "  'start': 32,\n",
       "  'end': 37}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipe = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"outputs/bert-tr-ner/final\",\n",
    "    tokenizer=\"outputs/bert-tr-ner/final\",\n",
    "    aggregation_strategy=\"simple\"   # merges sub-tokens into whole entities\n",
    ")\n",
    "\n",
    "text = \"Japonya Başbakanı Fumio Kishida bugün Ankara'da Cumhurbaşkanı ile görüştü.\"\n",
    "preds = ner_pipe(text)\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929d28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
