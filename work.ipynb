{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/yeniguno/projects/sugardata')\n",
    "\n",
    "import sugardata as su\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b90e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85e648",
   "metadata": {},
   "source": [
    "BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a55f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Access label mappings\n",
    "label2id = model.config.label2id\n",
    "id2label = model.config.id2label\n",
    "\n",
    "print(\"label2id:\", label2id)\n",
    "print(\"id2label:\", id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LABEL_MAPPING = {\n",
    "    \"LABEL_0\": \"Negative\",\n",
    "    \"LABEL_1\": \"Neutral\",\n",
    "    \"LABEL_2\": \"Positive\",\n",
    "    \"Negative\": \"LABEL_0\",\n",
    "    \"Neutral\": \"LABEL_1\",\n",
    "    \"Positive\": \"LABEL_2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=BASE_MODEL, device=0)\n",
    "\n",
    "pred_good = pipe(\"Good\")\n",
    "pred_bad = pipe(\"Bad\")\n",
    "pred_neutral = pipe(\"Neutral\")\n",
    "\n",
    "print(f\"Positive prediction: {pred_good}\")\n",
    "print(f\"Negative prediction: {pred_bad}\")\n",
    "print(f\"Neutral prediction: {pred_neutral}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f03e9e",
   "metadata": {},
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9120297",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"zeroshot/twitter-financial-news-sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2a16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(DATASET)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eeed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ds = ds[\"train\"].train_test_split(test_size=0.1, seed=42, shuffle=True)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": split_ds[\"train\"],\n",
    "    \"validation\": split_ds[\"test\"],  # this is your validation for fine-tuning\n",
    "    \"test\": ds[\"validation\"],        # your real validation set, now used for testing\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e685b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distribution(ds, feature_name: str):\n",
    "    # 1) Collect value‐counts for each split\n",
    "    counts = {}\n",
    "    all_categories = set()\n",
    "    for split_name, split in ds.items():\n",
    "        df = split.to_pandas()\n",
    "        vc = df[feature_name].value_counts()\n",
    "        counts[split_name] = vc\n",
    "        all_categories.update(vc.index.tolist())\n",
    "    \n",
    "    # 2) Build a DataFrame: rows=categories, cols=splits\n",
    "    categories = sorted(all_categories)\n",
    "    df_counts = pd.DataFrame(\n",
    "        { split: counts[split].reindex(categories, fill_value=0)\n",
    "          for split in counts },\n",
    "        index=categories\n",
    "    )\n",
    "    print(\"Counts DataFrame:\\n\", df_counts)\n",
    "    \n",
    "    # 3) Transpose for plotting: rows=splits, cols=categories\n",
    "    df_plot = df_counts.T\n",
    "    \n",
    "    # 4) Plot grouped bar chart with splits on x-axis\n",
    "    x = np.arange(len(df_plot.index))\n",
    "    n_categories = len(df_plot.columns)\n",
    "    width = 0.8 / n_categories\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    for i, category in enumerate(df_plot.columns):\n",
    "        ax.bar(x + i * width, df_plot[category], width, label=category)\n",
    "    \n",
    "    ax.set_xticks(x + width * (n_categories - 1) / 2)\n",
    "    ax.set_xticklabels(df_plot.index, rotation=0)\n",
    "    ax.set_xlabel(\"Split\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"Distribution of `{feature_name}` across splits\")\n",
    "    ax.legend(title=feature_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_distribution(ds, \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480890e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LABEL_MAPPING = {\n",
    "    \"Positive\": 1,\n",
    "    \"Negative\": 0,\n",
    "    \"Neutral\": 2,\n",
    "    0: \"Negative\",\n",
    "    1: \"Positive\",\n",
    "    2: \"Neutral\",\n",
    "}\n",
    "\n",
    "DATASET_LABELS = [\n",
    "    id2label.get(0),\n",
    "    id2label.get(1),\n",
    "    id2label.get(2),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET TO MODEL LABEL MAPPING\n",
    "#label2id: {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2}\n",
    "#id2label: {0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2'}\n",
    "\"\"\"\n",
    "Dataset -> Model Text -> Model Label\n",
    "0   -> Negative -> LABEL_0 -> 0\n",
    "1   -> Positive -> LABEL_2 -> 2\n",
    "2   -> Neutral  -> LABEL_1 -> 1\n",
    "\"\"\"\n",
    "print(ds[\"train\"][144])\n",
    "def encode_labels(example):\n",
    "    model_text_label =MODEL_LABEL_MAPPING.get(DATASET_LABEL_MAPPING.get(example['label']))\n",
    "    example['label'] = label2id[model_text_label]\n",
    "    return example\n",
    "\n",
    "ds = ds.map(encode_labels)\n",
    "\n",
    "print(ds[\"train\"][144])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d67b2ae",
   "metadata": {},
   "source": [
    "BASE MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21383117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentiment_model(\n",
    "        pipe: pipeline,\n",
    "        data: Dataset,\n",
    "        text_field: str,\n",
    "        true_label_field: str,\n",
    "        verbose: bool = True):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for idx, row in enumerate(data):\n",
    "        text = row[text_field]\n",
    "        if isinstance(text, str) and len(text) > 0:\n",
    "            text = text.strip()\n",
    "            text = text[:2000]\n",
    "\n",
    "        pred = pipe(text)[0][\"label\"]\n",
    "        true = id2label[row[true_label_field]]\n",
    "\n",
    "        y_true.append(true)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "        if verbose and idx % 1000 == 0:\n",
    "            print(f\"Processed {idx} rows: {pred} vs {true}\")\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred,labels=DATASET_LABELS)\n",
    "\n",
    "    # Per-class F1 (optional, for analysis)\n",
    "    f1_per_class = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"f1_per_class_0\": f1_per_class[0],\n",
    "        \"f1_per_class_1\": f1_per_class[1],\n",
    "        \"f1_per_class_2\": f1_per_class[2],\n",
    "        \n",
    "    }\n",
    "\n",
    "base_pipe = pipeline(\"text-classification\", model=BASE_MODEL, device=0)\n",
    "\n",
    "base_test_evals = evaluate_sentiment_model(pipe, ds[\"test\"], \"text\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709fea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_test_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01715a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(cm, labels):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "plot_cm(base_test_evals[\"confusion_matrix\"], DATASET_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_evals = evaluate_sentiment_model(pipe, ds[\"train\"], \"text\", \"label\")\n",
    "\n",
    "print(base_train_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_failed_instances(data, pipe, text_field, true_label_field):\n",
    "    row_indices = []\n",
    "    preds = []\n",
    "\n",
    "    for idx, row in enumerate(data):\n",
    "        text = row[text_field]\n",
    "        if isinstance(text, str) and len(text) > 0:\n",
    "            text = text.strip()\n",
    "            text = text[:2000]\n",
    "\n",
    "        pred = pipe(text)[0][\"label\"]\n",
    "        true = id2label[row[true_label_field]]\n",
    "        \n",
    "        if pred != true:\n",
    "            row_indices.append(idx)\n",
    "            preds.append(label2id[pred])\n",
    "\n",
    "    return row_indices, preds\n",
    "\n",
    "failed_train_indices, failed_train_preds = get_failed_instances(ds[\"train\"], pipe, \"text\", \"label\")\n",
    "print(f\"Number of failed instances: {len(failed_train_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_failed = ds[\"train\"].select(failed_train_indices).to_pandas()\n",
    "df_train_failed[\"preds\"] = failed_train_preds\n",
    "\n",
    "df_train_failed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9928ea",
   "metadata": {},
   "source": [
    "FIRST FINE TUNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236aa0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'],\n",
    "                     padding='max_length',\n",
    "                     truncation=True,\n",
    "                     max_length=512)\n",
    "\n",
    "train_dataset = ds['train']\n",
    "test_dataset = ds['test']\n",
    "validation_dataset = ds['validation']\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_validation = validation_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_train = tokenized_train.remove_columns(['text'])\n",
    "tokenized_test = tokenized_test.remove_columns(['text'])\n",
    "tokenized_validation = tokenized_validation.remove_columns(['text'])\n",
    "\n",
    "tokenized_train.set_format('torch')\n",
    "tokenized_test.set_format('torch')\n",
    "tokenized_validation.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6122e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    precision_weighted = precision_score(labels, predictions, average='weighted')\n",
    "    recall_weighted = recall_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    # Per-class F1 (optional, for analysis)\n",
    "    f1_per_class = f1_score(labels, predictions, average=None)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision\": precision_weighted,\n",
    "        \"recall\": recall_weighted,\n",
    "        \"f1_per_class_0\": f1_per_class[0],\n",
    "        \"f1_per_class_1\": f1_per_class[1],\n",
    "        \"f1_per_class_2\": f1_per_class[2],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f653d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"round_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b36c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,  # Further reduce the learning rate\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,  # Continue with fewer epochs due to overfitting\n",
    "    weight_decay=0.1,  # Increase weight decay to regularize the model\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=1,  # Accumulate gradients to simulate larger batch sizes\n",
    "    lr_scheduler_type=\"linear\",  # Use a linear learning rate scheduler\n",
    "    logging_steps=500,  # Increase logging to monitor training closely\n",
    "    warmup_steps=500,  # Add a warmup phase to stabilize learning at the start\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n",
    "    max_grad_norm=1.0,  # Gradient clipping,\n",
    "    fp16=True,  # enable mixed precision\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=12,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# Early stopping callback with patience of 2 epochs\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # Stop training if no improvement after 2 epochs\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "\n",
    "# Trainer with early stopping callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback],\n",
    "\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=new_model,\n",
    "    tokenizer=new_tokenizer,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "new_test_evals = evaluate_sentiment_model(new_pipe, ds[\"test\"], \"text\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_test_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c90796",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(new_test_evals[\"confusion_matrix\"], DATASET_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca6fb7",
   "metadata": {},
   "source": [
    "SUGAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_train_indices, failed_train_preds = get_failed_instances(ds[\"train\"], new_pipe, \"text\", \"label\")\n",
    "print(f\"Number of failed instances: {len(failed_train_indices)}\")\n",
    "\n",
    "df_train_failed = ds[\"train\"].select(failed_train_indices).to_pandas()\n",
    "df_train_failed[\"preds\"] = failed_train_preds\n",
    "\n",
    "df_train_failed.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "\n",
    "for idx, row in df_train_failed.iterrows():\n",
    "    label = row[\"label\"]\n",
    "    if label == 1:\n",
    "        continue\n",
    "\n",
    "    examples.append(row[\"text\"])\n",
    "\n",
    "print(f\"Number of examples: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44488da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = su.generate_sentiment_data(\n",
    "    language=\"en\",\n",
    "    examples=examples,\n",
    "    label_options=[\"Positive\", \"Negative\"],\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "print(f\"Number of generated examples: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd4fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_train(results):\n",
    "    \"\"\"\n",
    "    Append new examples to the training set.\n",
    "    \"\"\"\n",
    "    new_examples = []\n",
    "    for res in results:\n",
    "        label = label2id.get(MODEL_LABEL_MAPPING.get(res[\"label\"]))\n",
    "        new_examples.append({\"text\": res[\"generated_text\"], \"label\": label})\n",
    "\n",
    "    new_dataset = Dataset.from_list(new_examples)\n",
    "    ds[\"train\"] = concatenate_datasets([ds[\"train\"], new_dataset])\n",
    "    return ds\n",
    "\n",
    "ds = append_to_train(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(ds, \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246bf127",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "\n",
    "for instance in ds[\"train\"]:\n",
    "    if instance[\"label\"] == 0:\n",
    "        examples.append(instance[\"text\"])\n",
    "\n",
    "print(f\"Number of negative examples: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = su.generate_sentiment_data(\n",
    "    language=\"en\",\n",
    "    examples=examples,\n",
    "    label_options=[\"Negative\"],\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "print(f\"Number of generated examples: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = append_to_train(results)\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9faec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "\n",
    "for instance in ds[\"train\"]:\n",
    "    if instance[\"label\"] == 2:\n",
    "        examples.append(instance[\"text\"])\n",
    "\n",
    "print(f\"Number of positive examples: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2083fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = su.generate_sentiment_data(\n",
    "    language=\"en\",\n",
    "    examples=examples,\n",
    "    label_options=[\"Positive\"],\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "print(f\"Number of generated examples: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a1da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = append_to_train(results)\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(ds, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f0d3e",
   "metadata": {},
   "source": [
    "SECOND FINE TUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedff127",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'],\n",
    "                     padding='max_length',\n",
    "                     truncation=True,\n",
    "                     max_length=512)\n",
    "\n",
    "train_dataset = ds['train']\n",
    "test_dataset = ds['test']\n",
    "validation_dataset = ds['validation']\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_validation = validation_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_train = tokenized_train.remove_columns(['text'])\n",
    "tokenized_test = tokenized_test.remove_columns(['text'])\n",
    "tokenized_validation = tokenized_validation.remove_columns(['text'])\n",
    "\n",
    "tokenized_train.set_format('torch')\n",
    "tokenized_test.set_format('torch')\n",
    "tokenized_validation.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b9f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"round_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea3e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,  # Further reduce the learning rate\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,  # Continue with fewer epochs due to overfitting\n",
    "    weight_decay=0.1,  # Increase weight decay to regularize the model\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=1,  # Accumulate gradients to simulate larger batch sizes\n",
    "    lr_scheduler_type=\"linear\",  # Use a linear learning rate scheduler\n",
    "    logging_steps=500,  # Increase logging to monitor training closely\n",
    "    warmup_steps=500,  # Add a warmup phase to stabilize learning at the start\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n",
    "    max_grad_norm=1.0,  # Gradient clipping,\n",
    "    fp16=True,  # enable mixed precision\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=12,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# Early stopping callback with patience of 2 epochs\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # Stop training if no improvement after 2 epochs\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "\n",
    "# Trainer with early stopping callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback],\n",
    "\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa1e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=new_model,\n",
    "    tokenizer=new_tokenizer,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "new_test_evals = evaluate_sentiment_model(new_pipe, ds[\"test\"], \"text\", \"label\")\n",
    "\n",
    "print(new_test_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c426c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(new_test_evals[\"confusion_matrix\"], DATASET_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc150bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
